{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning for Biology\n",
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used:\n",
    "* **Regression problems**: Mean Absolute Error, Mean Squared Error, Root Mean Squared Error\n",
    "* **Classification problems**: Classification accuracy (so far)--> there are lots of other ways to evaluate classifiers, and we'll learn them now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the classification accuracy of a logistic regression fit to the cancer dataset\n",
    "Classification accuracy is the proportion of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Accuracy\n",
    "The accuracy that can be achieved by always predicting the most frequent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification accuracy is the easiest classification metric to understand\n",
    "* But, it does not tell you the underlying distribution of response values\n",
    "* And, it does not tell you what \"types\" of errors your classifier is making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "Table that describes the performance of a classification model\n",
    "\n",
    "<img src=\"assets/confusionmatrix.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the confusion matrix for the logistic regression of the cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every observation in the testing set is represented in exactly one box\n",
    "It's a 2x2 matrix because there are 2 response classes\n",
    "It tallies how many of the two types of correct predictions were made and the two types of incorrect predictions were made.\n",
    "#### Basic terminology\n",
    "\n",
    "* **True Positives (TP):** we correctly predicted that the sample is malignant\n",
    "* **True Negatives (TN):** we correctly predicted that the sample is benign\n",
    "* **False Positives (FP):** we incorrectly predicted that the sample is malignant (a \"Type I error\")\n",
    "* **False Negatives (FN):** we incorrectly predicted that the sample is benign (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculated from the Confusion Matrix\n",
    "<img src=\"assets/confusionmatrixmetrics.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification accuracy: ** overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Error:** Overall, how often is the classifier incorrect?\n",
    "* Also known as \"Misclassification Rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sensitivity:** When the actual value is positive, how often is the prediction correct?\n",
    "* How \"sensitive\" is the classifier to detecting positive instances?\n",
    "* Also known as \"True Positive Rate\" or \"Recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
    "* How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
    "* How \"precise\" is the classifier when predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demystifying Precision and Recall\n",
    "<img src=\"assets/precisionrecall.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Recall, Low Precision\n",
    "<img src=\"assets/lowPlowR.png\"/>\n",
    "<img src=\"assets/legen.jpeg\"/>\n",
    "* Let’s say everything inside the solid lines are pictures of actual hot dogs. \n",
    "* Everything within the dotted line is what the model thought was a picture of hot dogs.\n",
    "* Everything in the square is the entire dataset. \n",
    "* True negatives (denoted tn) samples in your data, which you classified as not belonging to your class correctly. Eg. your “hot dog” vs “not hot dog” image classifier correctly classified your image of a car as not being a “hot dog”.\n",
    "* False negatives (denoted fn) samples in your data, which you classified as not belonging to your class, incorrectly. Eg. your “hot dog” vs “not hot dog” image classifier incorrectly classified an image of a messed up “hot dog” as not being a “hot dog”.\n",
    "* True positives (denoted tp) samples in your data, which you classifed as belonging to your class correctly. Eg. your “hot dog” vs “not hot dog” classifier correctly classifies a “hot dog” as being a “hot dog”.\n",
    "* False positives (denoted fp) samples in your data, which you classified as belonging to your class incorrectly. Eg. your “hot dog” vs “not hot dog” classifier incorrectly classifies a hamburger as being a “hot dog”.\n",
    "\n",
    "#### High Recall, Low Precision\n",
    "<img src=\"assets/highRlowP.png\"/>\n",
    "* Our classifier casts a very wide net, catches a lot of fish, but also a lot of other things.\n",
    "* Our classifier thinks a lot of things are “hot dogs”; legs on beaches, fries and whatnot. \n",
    "* However it also thinks a lot of “hot dogs” are “hot dogs”. \n",
    "* So from our set of images we got a lot of images classified as “hot dogs”, many of them was in the set of actual “hot dogs”, however a lot of them were also “not hot dogs”.\n",
    "\n",
    "#### Low Recall, High Precision\n",
    "<img src=\"assets/lowRhighP.png\"/>\n",
    "* Our classifier casts a very small but highly specialized net, does not catch a lot of fish, but there is almost only fish in the net.\n",
    "* Our classifier is very picky, and does not think many things are hot dogs. \n",
    "* All the images it thinks are “hot dogs”, are really “hot dogs”. \n",
    "* However it also misses a lot of actual “hot dogs”, because it is so very picky. \n",
    "\n",
    "#### High Recall, High Precision\n",
    "<img src=\"assets/highRhighP.png\"/>\n",
    "* The holy grail, our fish net is wide and highly specialised. \n",
    "* We catch a lot of fish (almost all of it) and we almost get only fish, nothing else.\n",
    "* Our classifier is very good, it is very picky, but still it gets almost all of the images of “hot dogs” which are “hot dogs” correct. \n",
    "* We are happy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other metrics can be computed as well: F1 score, Matthews correlation coefficient, etc.\n",
    "\n",
    "**Conclusion:**\n",
    "* Confusion matrix gives you a more complete picture of how your classifier is performing\n",
    "* Also allows you to compute various classification metrics, and these metrics can guide your model selection\n",
    "\n",
    "**Which metrics should you focus on?**\n",
    "* Choice of metric depends on your objective\n",
    "* For example, a spam filter (positive class is \"spam\"): Optimize for precision or specificity because false negatives (spam goes to the inbox) are more acceptable than false positives (non-spam is caught by the spam filter)\n",
    "* Or for fraudulent transaction detector (positive class is \"fraud\"): Optimize for sensitivity because false positives (normal transactions that are flagged as possible fraud) are more acceptable than false negatives (fraudulent transactions that are not detected)\n",
    "\n",
    "***Which matters more for your research problem?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the classification threshold\n",
    "Like a metal detector being adjusted to look for larger and smaller bits of metal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `.predict` is using these probabilities to choose which class it predicts (if greater than 50%). If we adjust this threshold, we can adjust the sensitivity and specificity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decrease the threshold for predicting malignancy to increase the sensitivity of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Threshold of 0.5 is used by default (for binary problems) to convert predicted probabilities into class predictions\n",
    "* Threshold can be adjusted to increase sensitivity or specificity\n",
    "* Sensitivity and specificity have an inverse relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Exercise\n",
    "* Calculate the confusion matrix for the diabetes dataset after doing a logistic regression.\n",
    "\n",
    "\n",
    "**Bonus:** try adjusting the classification threshold. Does it change the sensitivity and specificity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and Area Under the Curve (AUC)\n",
    "*Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?*\n",
    "\n",
    "**ROC curves to the rescue!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receiver Operator Curves** tell us about the false positive and true positive rates. A diagonal line would be the same as random guessing. If the ROC curve falls below the diagonal, it is worse than random guessing. A perfect classifier would fall on the edge of the top left corner (true positive rate of 1, false positive rate of 0). The **Area Under the Curve (AUC)** score tells us about the performance of the model. \n",
    "\n",
    "<img src=\"assets/roc.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the AUC for the cancer dataset\n",
    "AUC is the percentage of the ROC plot that is underneath the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AUC is useful as a single number summary of classifier performance.\n",
    "* If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation.\n",
    "* AUC is useful even when there is high class imbalance (unlike classification accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC curve can help you to choose a threshold that balances sensitivity and specificity in a way that makes sense for your particular context\n",
    "* You can't actually see the thresholds used to generate the curve on the ROC curve itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Independent Practice\n",
    "Plot the ROC Curve and calculate the AUC for the diabetes dataset for a logistic regression. \n",
    "\n",
    "*What classification accuracy threshold do you recommend?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Confusion matrix advantages:**\n",
    "* Allows you to calculate a variety of metrics\n",
    "* Useful for multi-class problems (more than two response classes)\n",
    "\n",
    "\n",
    "**ROC/AUC advantages:**\n",
    "* Does not require you to set a classification threshold\n",
    "* Still useful when there is high class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics: Learning and Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing bias and variance with learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves plot the number of training samples and accuracy for both training and validation.  \n",
    "\n",
    "Models with high bias in both training and validation sets indicate overfitting. Increasing number of model parameters or decreasing regularization can help.  \n",
    "\n",
    "Models with high variance (big gap between training and validation sets are overfit. Collecting more data, reducing the complexity of the model, or increasing regularization can help.\n",
    "<img src=\"assets/learningcurves.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curves for the cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([\n",
    "    ('scl', StandardScaler()), \n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores =\\\n",
    "    learning_curve(estimator=pipe_lr,\n",
    "                  X=X_train,\n",
    "                  y=y_train, \n",
    "                  train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                  cv=10,\n",
    "                  n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis = 1)\n",
    "\n",
    "train_std = np.std(train_scores, axis = 1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "test_std = np.std(test_scores, axis = 1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean-train_std, alpha=0.15, color = 'blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, color = 'green', linestyle = '--', marker = 's', markersize=5, label = 'validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean-test_std, alpha=0.15, color = 'green')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Number of training samples')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "plt.ylim([0.8, 1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our model look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing overfitting and underfitting with validation curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation curves are like learning curves, but instead of plotting training and validation accuracies as a function of sample size, we vary the values of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot validation curves for the cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    estimator = pipe_lr, \n",
    "    X = X_train,\n",
    "    y=y_train,\n",
    "    param_name = 'clf__C',\n",
    "    param_range = param_range,\n",
    "    cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean =np.mean(train_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_std = np.std(train_scores, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_std = np.std(test_scores, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(param_range, train_mean, color='blue', marker = 'o', markersize = 5, label = 'training accuracy')\n",
    "plt.fill_between(param_range, train_mean + train_std, train_mean-train_std, alpha=0.15, color = 'blue')\n",
    "plt.plot(param_range, test_mean, color='green', marker = 's', markersize = 5, label = 'validation accuracy')\n",
    "plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which levels of C seem best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
